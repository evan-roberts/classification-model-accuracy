{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the accuracy of your classification model\n",
    "\n",
    "#### Use accuracy score, confusion matrix, and F1-score to check how accurate your classification model is.\n",
    "\n",
    "In the previous post we [built a decision tree model with scikit-learn](https://theleftjoin.com/build-a-decision-tree-model-with-scikit-learn/). It attempts to predict which customers had life insurance based on their income and property status.\n",
    "\n",
    "Step 7 of the development process is to **check the accuracy of the model**. Which is what we’ll look at here.\n",
    "\n",
    "We already split our data into [train and test sets](https://theleftjoin.com/split-your-data-into-train-and-test-sets/) before fitting the model. we can now use the **test set** to see how well the model performs.\n",
    "\n",
    "For demonstration purposes, the **output test set** `y_test` has 6 records. These are the actual output values which correspond to the **input test set** `X_test`. Let's take a look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     0\n",
       "17    0\n",
       "1     1\n",
       "9     1\n",
       "11    0\n",
       "5     1\n",
       "Name: has_life_insurance, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"life_insurance_data.csv\")\n",
    "\n",
    "df = pd.get_dummies(df, columns=['property_status'], prefix=['property_status'])\n",
    "\n",
    "X = df.drop(columns=['has_life_insurance'])\n",
    "y = df['has_life_insurance']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "model = tree.DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see what the model predicted for the test input `X_test`. That is, what the model predicted for `has_life_insurance` given its inputs `income_usd` and `property_status`. We can do this using `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick glance at the data shows that the model predicted **5 out of 6 cases correctly**, with only item 3 being incorrectly classified.\n",
    "\n",
    "## Accuracy score\n",
    "\n",
    "Accuracy score is the number of **correct predictions** divided by the **total number of predictions**.\n",
    "\n",
    "It’s an intuitive measure that’s easy to understand. In fact, we’ve already calculated it above when we said that the model predicted **5 out of 6**, or **83.3%** of cases correctly.\n",
    "\n",
    "An easy way to calculate this in Python is with `accuracy_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score = accuracy_score(y_test, y_predicted)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low accuracy score is a sign that there are some issues with the model. You may want to increase your sample size or retrain using a different algorithm.\n",
    "\n",
    "## Confusion matrix\n",
    "\n",
    "A confusion matrix simply shows you where any differences are coming from, i.e. are there certain areas of your model which are misclassifying?\n",
    "\n",
    "Let’s take a look at our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted: 0</th>\n",
       "      <th>predicted: 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual: 0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual: 1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           predicted: 0  predicted: 1\n",
       "actual: 0             3             0\n",
       "actual: 1             1             2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_predicted), \n",
    "    index=['actual: 0', 'actual: 1'], \n",
    "    columns=['predicted: 0', 'predicted: 1']\n",
    ")\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix above shows **how many observations** were in each predicted and actual classification. The cells highlighted in yellow are where the model predicted the `has_life_insurance` flag correctly.\n",
    "\n",
    "Each cell in the matrix can be classified like this:\n",
    "\n",
    "<div style=\"color: black\">\n",
    "  <table>\n",
    "    <tr>\n",
    "      <th style=\"background-color:#F9F9F9\"></th>\n",
    "      <th colspan=\"2\" style=\"background-color:#F0F0F0\">predicted</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"background-color:#F9F9F9\"><strong>actual</strong></td>\n",
    "      <td style=\"background-color:#F0F0F0\"><strong>0</strong></td>\n",
    "      <td style=\"background-color:#F0F0F0\"><strong>1</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"background-color:#F9F9F9\"><strong>0</strong></td>\n",
    "      <td style=\"background-color:#FFEDC0\">True negative</td>\n",
    "      <td style=\"background-color:white\">False positive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"background-color:#F9F9F9 \"><strong>1</strong></td>\n",
    "      <td style=\"background-color:white\">False negative</td>\n",
    "      <td style=\"background-color:#FFEDC0\">True positive</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "If there are a high number of false negatives or false positives then you can focus your attention on fixing those cases in your model.\n",
    "\n",
    "## F1-score\n",
    "\n",
    "The **F1-score** for a model is calculated using **precision** and **recall**.\n",
    "\n",
    "For a binary classifier like we have here, the precision, recall, and F1-score for the model can be calculated like this.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision is the percentage of positive predictions which were correct.\n",
    "\n",
    "I.e. true positives as a percentage of all the positive predictions - column `predicted = 1` in the confusion matrix.\n",
    "\n",
    "\n",
    "```\n",
    "Precision = True positive / (True positive + False positive)\n",
    "          = 2 / (2 + 0)\n",
    "          = 1.00\n",
    "```\n",
    "\n",
    "### Recall\n",
    "\n",
    "Recall is the percentage of actual positive values which were predicted correctly.\n",
    "\n",
    "I.e. the percentage of true positives in the row where `actual = 1`.\n",
    "\n",
    "```\n",
    "Recall = True positive / (True positive + False negative)\n",
    "       = 2 / (2 + 1)\n",
    "       = 0.67\n",
    "```\n",
    "\n",
    "### F1-score\n",
    "\n",
    "The F1-score ranges from **0 to 1**, with 1 being the best and 0 being the worst.\n",
    "\n",
    "F1-score is the harmonic mean of **precision** and **recall**.\n",
    "\n",
    "```\n",
    "F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "         = 2 * (1.00 * 0.67) / (1.00 + 0.67)\n",
    "         = 0.80\n",
    "```\n",
    "\n",
    "For a model which classifies into more than one category, the precision, recall, and F1-score of each class can be calculated. Then an average of these F1-scores can be used as a score for the entire model - more on this below.\n",
    "\n",
    "## Classification report\n",
    "\n",
    "If all these calculations seem laborious, don’t worry! These all get calculated for you with `classification_report()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our case, where we only have a binary output, `0` or `1`, you should read the row with `1` for the model’s overall precision, recall, and F1-score.\n",
    "\n",
    "You can see that the model has an F1-score of 0.8, which is what we calculated manually above.\n",
    "\n",
    "If we had a multi-class model (i.e. more output options than just `0` and `1`), then you could use the weighted average of each class's F1-score as a score for the model. This takes into account the number of actual observations for each class, which is shown under the `support` column. This is calculated for you in the `weighted avg` row.\n",
    "\n",
    "## Accuracy score vs. F1-score\n",
    "\n",
    "One of the benefits of using accuracy score is that it's easy to interpret. If a model predicts 95% of the classifications correctly, then the accuracy score will be 95%.\n",
    "\n",
    "However, this can be a problem for cases where the model is predicting something that actually happens 95% of the time.\n",
    "\n",
    "For example, the probability that a patient is healthy. If the model simply says that every patient patient is healthy regardless of the inputs, it would still have a high accuracy score of 95% as it would be correct 95% of the time!\n",
    "\n",
    "This is where F1-score comes in useful, as it takes into account how the data is distributed between true/false positives and negatives. Therefore, it’s a good idea to use F1-score when we can see a large imbalance between the groups on a confusion matrix."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
